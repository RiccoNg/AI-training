import pandas as pd
import joblib
import matplotlib.pyplot as plt
import os
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    auc,
    classification_report
)
from xgboost import XGBClassifier
from utils.data_loader import download_and_process_data
from utils.strategy_simulator import simulate_strategy
from utils.strategy_simulator import optimize_strategy

# å»ºç«‹ model è³‡æ–™å¤¾ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
os.makedirs("model", exist_ok=True)

# 1. ä¸‹è¼‰èˆ‡è™•ç†è³‡æ–™
yf_ticker = "AAPL"  # å¯æ›¿æ›æˆå…¶ä»–è‚¡ç¥¨ä»£ç¢¼
df = download_and_process_data(yf_ticker)

# 2. æŒ‡å®šä½¿ç”¨çš„æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ
features = [
    "MA5", "MA10", "EMA12", "EMA26",
    "Return", "Volatility", "RSI",
    "MACD", "MACD_signal",
    "BB_upper", "BB_lower",
    "Momentum"
]

X = df[features]
y = df["target"]

# 3. æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼ˆå‰ 80% ç‚ºè¨“ç·´é›†ï¼‰
split_index = int(len(df) * 0.8)
X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

# 4. æ¨¡å‹è¨“ç·´ï¼ˆXGBoostï¼‰
model = XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    eval_metric="logloss"
)
model.fit(X_train, y_train)

# 5. æ¨¡å‹å„²å­˜
joblib.dump(model, "model/xgb_model.pkl")
print("âœ… æ¨¡å‹å·²å„²å­˜è‡³ model/xgb_model.pkl")

# 6. é æ¸¬èˆ‡æº–ç¢ºç‡
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"ğŸ“ˆ æ¨¡å‹æº–ç¢ºç‡ï¼š{accuracy:.2f}")

# 7. æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot()
plt.title("Confusion Matrix")
plt.show()

# 8. ROC æ›²ç·šèˆ‡ AUC
y_prob = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

# 9. åˆ†é¡å ±å‘Š
print("ğŸ“Š åˆ†é¡å ±å‘Šï¼š")
print(classification_report(y_test, y_pred))

# 10. ç‰¹å¾µé‡è¦æ€§åˆ†æ
importances = model.feature_importances_
plt.figure(figsize=(8, 6))
plt.barh(features, importances)
plt.title("Feature Importance")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# 12. ç­–ç•¥æ¨¡æ“¬ï¼ˆå–®æ¬¡å›æ¸¬ + ç•«è³‡é‡‘æ›²ç·šï¼‰
simulate_strategy(
    df=df,
    y_pred=y_pred,
    y_prob=y_prob,
    fee=0.0004,           # æ‰‹çºŒè²»
    stop_loss=-0.05,      # åœæ -5%
    take_profit=0.05,     # åœåˆ© +5%
    hold_days=3,          # æŒæœ‰å¤©æ•¸
    dynamic_position=False,
    split_index=split_index,
    plot_curve=True,
    verbose=True
)

# 13. åƒæ•¸å„ªåŒ–
param_grid = {
    "fee": [0.0002, 0.0004],
    "stop_loss": [-0.03, -0.05],
    "take_profit": [0.03, 0.05],
    "hold_days": [1, 3, 5],
    "dynamic_position": [False, True]
}
best_params, best_metrics = optimize_strategy(
    df=df,
    y_pred=y_pred,
    y_prob=y_prob,
    split_index=split_index,
    initial_capital=30000,
    param_grid=param_grid
)

print("\nğŸ¯ æœ€ä½³å„ªåŒ–çµæœï¼š")
print(best_params)
print(best_metrics)


